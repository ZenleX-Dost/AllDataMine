{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01b32c9",
   "metadata": {},
   "source": [
    "# DATA MINING PROJECT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bce77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.0.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (3.9.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\amine\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\amine\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn xgboost tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c85b3bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from joblib import parallel_backend\n",
    "import xgboost as xgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622dedf",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb68571",
   "metadata": {},
   "source": [
    "### Define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b397e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['FD001', 'FD002', 'FD003', 'FD004']\n",
    "train_data_dict = {}\n",
    "test_data_dict = {}\n",
    "true_rul_dict = {}\n",
    "\n",
    "columns = ['unit_number', 'time_cycles'] + \\\n",
    "          [f'setting_{i}' for i in range(1, 4)] + \\\n",
    "          [f'sensor_{i}' for i in range(1, 22)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df519ff4",
   "metadata": {},
   "source": [
    "### Check for file existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f6bd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    if not os.path.exists(f'train_{dataset}.txt'):\n",
    "        raise FileNotFoundError(f\"train_{dataset}.txt not found\")\n",
    "    if not os.path.exists(f'test_{dataset}.txt'):\n",
    "        raise FileNotFoundError(f\"test_{dataset}.txt not found\")\n",
    "    if not os.path.exists(f'RUL_{dataset}.txt'):\n",
    "        raise FileNotFoundError(f\"RUL_{dataset}.txt not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78b60f",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1840432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    train_data = pd.read_csv(f'train_{dataset}.txt', sep='\\s+', header=None, names=columns)\n",
    "    test_data = pd.read_csv(f'test_{dataset}.txt', sep='\\s+', header=None, names=columns)\n",
    "    true_rul = pd.read_csv(f'RUL_{dataset}.txt', header=None, names=['true_rul'])\n",
    "    \n",
    "    # Ensure consistent columns\n",
    "    train_data = train_data[columns]\n",
    "    test_data = test_data[columns]\n",
    "    \n",
    "    # Store in dictionaries\n",
    "    train_data_dict[dataset] = train_data\n",
    "    test_data_dict[dataset] = test_data\n",
    "    true_rul_dict[dataset] = true_rul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dba717",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "918314c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in FD001 training data:\n",
      " unit_number    0\n",
      "time_cycles    0\n",
      "setting_1      0\n",
      "setting_2      0\n",
      "setting_3      0\n",
      "sensor_1       0\n",
      "sensor_2       0\n",
      "sensor_3       0\n",
      "sensor_4       0\n",
      "sensor_5       0\n",
      "sensor_6       0\n",
      "sensor_7       0\n",
      "sensor_8       0\n",
      "sensor_9       0\n",
      "sensor_10      0\n",
      "sensor_11      0\n",
      "sensor_12      0\n",
      "sensor_13      0\n",
      "sensor_14      0\n",
      "sensor_15      0\n",
      "sensor_16      0\n",
      "sensor_17      0\n",
      "sensor_18      0\n",
      "sensor_19      0\n",
      "sensor_20      0\n",
      "sensor_21      0\n",
      "max_cycle      0\n",
      "RUL            0\n",
      "dtype: int64\n",
      "Missing values in FD002 training data:\n",
      " unit_number    0\n",
      "time_cycles    0\n",
      "setting_1      0\n",
      "setting_2      0\n",
      "setting_3      0\n",
      "sensor_1       0\n",
      "sensor_2       0\n",
      "sensor_3       0\n",
      "sensor_4       0\n",
      "sensor_5       0\n",
      "sensor_6       0\n",
      "sensor_7       0\n",
      "sensor_8       0\n",
      "sensor_9       0\n",
      "sensor_10      0\n",
      "sensor_11      0\n",
      "sensor_12      0\n",
      "sensor_13      0\n",
      "sensor_14      0\n",
      "sensor_15      0\n",
      "sensor_16      0\n",
      "sensor_17      0\n",
      "sensor_18      0\n",
      "sensor_19      0\n",
      "sensor_20      0\n",
      "sensor_21      0\n",
      "max_cycle      0\n",
      "RUL            0\n",
      "dtype: int64\n",
      "Missing values in FD003 training data:\n",
      " unit_number    0\n",
      "time_cycles    0\n",
      "setting_1      0\n",
      "setting_2      0\n",
      "setting_3      0\n",
      "sensor_1       0\n",
      "sensor_2       0\n",
      "sensor_3       0\n",
      "sensor_4       0\n",
      "sensor_5       0\n",
      "sensor_6       0\n",
      "sensor_7       0\n",
      "sensor_8       0\n",
      "sensor_9       0\n",
      "sensor_10      0\n",
      "sensor_11      0\n",
      "sensor_12      0\n",
      "sensor_13      0\n",
      "sensor_14      0\n",
      "sensor_15      0\n",
      "sensor_16      0\n",
      "sensor_17      0\n",
      "sensor_18      0\n",
      "sensor_19      0\n",
      "sensor_20      0\n",
      "sensor_21      0\n",
      "max_cycle      0\n",
      "RUL            0\n",
      "dtype: int64\n",
      "Missing values in FD004 training data:\n",
      " unit_number    0\n",
      "time_cycles    0\n",
      "setting_1      0\n",
      "setting_2      0\n",
      "setting_3      0\n",
      "sensor_1       0\n",
      "sensor_2       0\n",
      "sensor_3       0\n",
      "sensor_4       0\n",
      "sensor_5       0\n",
      "sensor_6       0\n",
      "sensor_7       0\n",
      "sensor_8       0\n",
      "sensor_9       0\n",
      "sensor_10      0\n",
      "sensor_11      0\n",
      "sensor_12      0\n",
      "sensor_13      0\n",
      "sensor_14      0\n",
      "sensor_15      0\n",
      "sensor_16      0\n",
      "sensor_17      0\n",
      "sensor_18      0\n",
      "sensor_19      0\n",
      "sensor_20      0\n",
      "sensor_21      0\n",
      "max_cycle      0\n",
      "RUL            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    train_data = train_data_dict[dataset]\n",
    "    test_data = test_data_dict[dataset]\n",
    "    \n",
    "    # Calculate RUL for training data\n",
    "    train_data['max_cycle'] = train_data.groupby('unit_number')['time_cycles'].transform('max')\n",
    "    train_data['RUL'] = train_data['max_cycle'] - train_data['time_cycles']\n",
    "    \n",
    "    # Check missing values\n",
    "    print(f\"Missing values in {dataset} training data:\\n\", train_data.isnull().sum())\n",
    "    \n",
    "    # Normalize settings and sensors\n",
    "    scale_cols = [f'setting_{i}' for i in range(1, 4)] + [f'sensor_{i}' for i in range(1, 22)]\n",
    "    scaler = MinMaxScaler()\n",
    "    train_data[scale_cols] = scaler.fit_transform(train_data[scale_cols])\n",
    "    test_data[scale_cols] = scaler.transform(test_data[scale_cols])\n",
    "    \n",
    "    # Cluster operating conditions for FD002 and FD004\n",
    "    if dataset in ['FD002', 'FD004']:\n",
    "        setting_cols = [f'setting_{i}' for i in range(1, 4)]\n",
    "        kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "        train_data['op_condition'] = kmeans.fit_predict(train_data[setting_cols])\n",
    "        test_data['op_condition'] = kmeans.predict(test_data[setting_cols])\n",
    "    else:\n",
    "        train_data['op_condition'] = 0  # Single condition for FD001, FD003\n",
    "        test_data['op_condition'] = 0\n",
    "    \n",
    "    # Update dictionaries\n",
    "    train_data_dict[dataset] = train_data\n",
    "    test_data_dict[dataset] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd2ff7c",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4e5d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 sensors correlated with RUL in FD001:\n",
      " sensor_12    0.671983\n",
      "sensor_7     0.657223\n",
      "sensor_21    0.635662\n",
      "sensor_20    0.629428\n",
      "sensor_6    -0.128348\n",
      "Name: RUL, dtype: float64\n",
      "Top 5 sensors correlated with RUL in FD002:\n",
      " sensor_20    0.006287\n",
      "sensor_21    0.006165\n",
      "sensor_19    0.005761\n",
      "sensor_13    0.005245\n",
      "sensor_18    0.004780\n",
      "Name: RUL, dtype: float64\n",
      "Top 5 sensors correlated with RUL in FD003:\n",
      " sensor_20    0.037782\n",
      "sensor_21    0.033465\n",
      "sensor_15   -0.016501\n",
      "sensor_6    -0.215352\n",
      "sensor_7    -0.315048\n",
      "Name: RUL, dtype: float64\n",
      "Top 5 sensors correlated with RUL in FD004:\n",
      " sensor_20    0.002812\n",
      "sensor_21    0.002791\n",
      "sensor_18    0.002765\n",
      "sensor_19    0.002303\n",
      "sensor_8     0.002086\n",
      "Name: RUL, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    train_data = train_data_dict[dataset]\n",
    "    \n",
    "    # Sensor trends for Engine 1\n",
    "    engine1 = train_data[train_data['unit_number'] == 1]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(engine1['time_cycles'], engine1['sensor_2'], label='Sensor 2')\n",
    "    plt.plot(engine1['time_cycles'], engine1['sensor_3'], label='Sensor 3')\n",
    "    plt.xlabel('Time Cycles')\n",
    "    plt.ylabel('Scaled Sensor Value')\n",
    "    plt.title(f'Sensor Trends for Engine 1 - {dataset}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'sensor_trends_engine1_{dataset}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Correlation analysis\n",
    "    correlations = train_data[[f'sensor_{i}' for i in range(1, 22)] + ['RUL']].corr()\n",
    "    sensor_rul_corr = correlations['RUL'].drop('RUL').sort_values(ascending=False)\n",
    "    print(f\"Top 5 sensors correlated with RUL in {dataset}:\\n\", sensor_rul_corr.head())\n",
    "    \n",
    "    # Operating condition distribution (FD002/FD004)\n",
    "    if dataset in ['FD002', 'FD004']:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(x='op_condition', data=train_data)\n",
    "        plt.title(f'Operating Condition Distribution - {dataset}')\n",
    "        plt.savefig(f'op_condition_dist_{dataset}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfda951",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa07789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:23: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
      "C:\\Users\\amine\\AppData\\Local\\Temp\\ipykernel_2400\\2228989494.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n"
     ]
    }
   ],
   "source": [
    "window = 5\n",
    "sensors = [f'sensor_{i}' for i in range(1, 22)]\n",
    "\n",
    "for dataset in datasets:\n",
    "    train_data = train_data_dict[dataset]\n",
    "    test_data = test_data_dict[dataset]\n",
    "    \n",
    "    # Rolling statistics and lags\n",
    "    for sensor in sensors:\n",
    "        # Rolling mean and std, grouped by unit_number and op_condition\n",
    "        train_data[f'{sensor}_roll_mean'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()).fillna(train_data[sensor])\n",
    "        train_data[f'{sensor}_roll_std'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()).fillna(0)\n",
    "        test_data[f'{sensor}_roll_mean'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()).fillna(test_data[sensor])\n",
    "        test_data[f'{sensor}_roll_std'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()).fillna(0)\n",
    "        \n",
    "        # Lagged features\n",
    "        train_data[f'{sensor}_lag1'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(train_data[sensor])\n",
    "        train_data[f'{sensor}_lag2'] = train_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(train_data[sensor])\n",
    "        test_data[f'{sensor}_lag1'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(1).fillna(test_data[sensor])\n",
    "        test_data[f'{sensor}_lag2'] = test_data.groupby(['unit_number', 'op_condition'])[sensor].shift(2).fillna(test_data[sensor])\n",
    "    \n",
    "    # Update feature columns\n",
    "    feature_cols = scale_cols + ['op_condition'] + \\\n",
    "                   [f'{s}_roll_mean' for s in sensors] + \\\n",
    "                   [f'{s}_roll_std' for s in sensors] + \\\n",
    "                   [f'{s}_lag1' for s in sensors] + \\\n",
    "                   [f'{s}_lag2' for s in sensors]\n",
    "    \n",
    "    # Update dictionaries\n",
    "    train_data_dict[dataset] = train_data\n",
    "    test_data_dict[dataset] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73753819",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc45f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing FD001\n",
      "FD001 Random Forest CV Results - RMSE: 45.91, MAE: 31.99, R2: 0.55, Custom Score: 61254313960.62\n",
      "\n",
      "FD001 Test Set - RMSE: 39.62, MAE: 26.82, R2: 0.09, Custom Score: 83149.51\n",
      "\n",
      "Processing FD002\n",
      "FD002 Random Forest CV Results - RMSE: 42.65, MAE: 30.98, R2: 0.62, Custom Score: 54794329676.49\n",
      "\n",
      "FD002 Test Set - RMSE: 29.90, MAE: 21.44, R2: 0.69, Custom Score: 20151.55\n",
      "\n",
      "FD002 Top 10 Important Features:\n",
      "                 feature  importance\n",
      "15            sensor_13    0.153104\n",
      "28   sensor_4_roll_mean    0.100846\n",
      "39  sensor_15_roll_mean    0.082041\n",
      "35  sensor_11_roll_mean    0.080790\n",
      "17            sensor_15    0.051648\n",
      "13            sensor_11    0.049115\n",
      "53    sensor_8_roll_std    0.033812\n",
      "31   sensor_7_roll_mean    0.025133\n",
      "58   sensor_13_roll_std    0.017067\n",
      "56   sensor_11_roll_std    0.014888\n",
      "\n",
      "Processing FD003\n",
      "FD003 Random Forest CV Results - RMSE: 68.34, MAE: 46.71, R2: 0.50, Custom Score: 28642261420658.68\n",
      "\n",
      "FD003 Test Set - RMSE: 47.11, MAE: 32.67, R2: -0.30, Custom Score: 336217.95\n",
      "\n",
      "FD003 Top 10 Important Features:\n",
      "                 feature  importance\n",
      "35  sensor_11_roll_mean    0.497654\n",
      "36  sensor_12_roll_mean    0.093021\n",
      "30   sensor_6_roll_mean    0.057382\n",
      "33   sensor_9_roll_mean    0.050935\n",
      "31   sensor_7_roll_mean    0.034980\n",
      "38  sensor_14_roll_mean    0.023843\n",
      "37  sensor_13_roll_mean    0.011743\n",
      "28   sensor_4_roll_mean    0.010962\n",
      "32   sensor_8_roll_mean    0.008789\n",
      "52    sensor_7_roll_std    0.008610\n",
      "\n",
      "Processing FD004\n",
      "FD004 Random Forest CV Results - RMSE: 56.41, MAE: 40.21, R2: 0.60, Custom Score: 2304282992178032.00\n",
      "\n",
      "FD004 Test Set - RMSE: 40.22, MAE: 28.46, R2: 0.46, Custom Score: 594808.59\n",
      "\n",
      "FD004 Top 10 Important Features:\n",
      "                 feature  importance\n",
      "15            sensor_13    0.275208\n",
      "35  sensor_11_roll_mean    0.075366\n",
      "17            sensor_15    0.050845\n",
      "39  sensor_15_roll_mean    0.041832\n",
      "30   sensor_6_roll_mean    0.039174\n",
      "28   sensor_4_roll_mean    0.037291\n",
      "13            sensor_11    0.032726\n",
      "26   sensor_2_roll_mean    0.021994\n",
      "53    sensor_8_roll_std    0.016001\n",
      "11             sensor_9    0.015653\n"
     ]
    }
   ],
   "source": [
    "def custom_score(y_true, y_pred):\n",
    "    d = y_pred - y_true\n",
    "    return np.sum(np.where(d < 0, np.exp(-d / 10) - 1, np.exp(d / 13) - 1))\n",
    "\n",
    "results = {}\n",
    "top_features = {\n",
    "    'FD001': ['sensor_4_roll_mean', 'sensor_9_roll_mean', 'sensor_21_roll_mean', 'sensor_11_roll_mean', 'sensor_14_roll_mean'],\n",
    "    'FD002': [],  # Add based on feature importance from initial run\n",
    "    'FD003': [],  # Add based on feature importance from initial run\n",
    "    'FD004': []   # Add based on feature importance from initial run\n",
    "}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\nProcessing {dataset}\")\n",
    "    train_data = train_data_dict[dataset]\n",
    "    test_data = test_data_dict[dataset]\n",
    "    true_rul = true_rul_dict[dataset]\n",
    "    \n",
    "    # Use top features if available, else all features\n",
    "    current_features = top_features[dataset] if top_features[dataset] else feature_cols\n",
    "    \n",
    "    # Prepare data\n",
    "    engines = train_data['unit_number'].unique()\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)  # Reduced to 3 folds\n",
    "    \n",
    "    # Initialize Random Forest with parallel processing\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)  # Reduced trees, parallelized\n",
    "    \n",
    "    # Cross-validation\n",
    "    rmse_scores, mae_scores, r2_scores, custom_scores = [], [], [], []\n",
    "    with parallel_backend('threading'):  # Ensure parallel processing\n",
    "        for train_idx, val_idx in kf.split(engines):\n",
    "            train_engines = engines[train_idx]\n",
    "            val_engines = engines[val_idx]\n",
    "            train_df = train_data[train_data['unit_number'].isin(train_engines)]\n",
    "            val_df = train_data[train_data['unit_number'].isin(val_engines)]\n",
    "            \n",
    "            X_train = train_df[current_features]\n",
    "            y_train = train_df['RUL']\n",
    "            X_val = val_df[current_features]\n",
    "            y_val = val_df['RUL']\n",
    "            \n",
    "            rf.fit(X_train, y_train)\n",
    "            y_pred = rf.predict(X_val)\n",
    "            \n",
    "            rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "            mae_scores.append(mean_absolute_error(y_val, y_pred))\n",
    "            r2_scores.append(r2_score(y_val, y_pred))\n",
    "            custom_scores.append(custom_score(y_val, y_pred))\n",
    "    \n",
    "    model_results = {\n",
    "        'Random Forest': {\n",
    "            'RMSE': np.mean(rmse_scores),\n",
    "            'MAE': np.mean(mae_scores),\n",
    "            'R2': np.mean(r2_scores),\n",
    "            'Custom Score': np.mean(custom_scores)\n",
    "        }\n",
    "    }\n",
    "    print(f\"{dataset} Random Forest CV Results - RMSE: {model_results['Random Forest']['RMSE']:.2f}, \"\n",
    "          f\"MAE: {model_results['Random Forest']['MAE']:.2f}, R2: {model_results['Random Forest']['R2']:.2f}, \"\n",
    "          f\"Custom Score: {model_results['Random Forest']['Custom Score']:.2f}\")\n",
    "    \n",
    "    # Train final model on full training data\n",
    "    X_train = train_data[current_features]\n",
    "    y_train = train_data['RUL']\n",
    "    with parallel_backend('threading'):\n",
    "        rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    test_last = test_data.groupby('unit_number').last().reset_index()\n",
    "    X_test = test_last[current_features]\n",
    "    y_test_pred = rf.predict(X_test)\n",
    "    \n",
    "    rmse_test = np.sqrt(mean_squared_error(true_rul['true_rul'], y_test_pred))\n",
    "    mae_test = mean_absolute_error(true_rul['true_rul'], y_test_pred)\n",
    "    r2_test = r2_score(true_rul['true_rul'], y_test_pred)\n",
    "    score_test = custom_score(true_rul['true_rul'], y_test_pred)\n",
    "    \n",
    "    print(f\"\\n{dataset} Test Set - RMSE: {rmse_test:.2f}, MAE: {mae_test:.2f}, R2: {r2_test:.2f}, \"\n",
    "          f\"Custom Score: {score_test:.2f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[dataset] = {'CV': model_results, 'Test': {'RMSE': rmse_test, 'MAE': mae_test, 'R2': r2_test, 'Custom Score': score_test}}\n",
    "    \n",
    "    # Feature importance (only for first run to populate top_features)\n",
    "    if not top_features[dataset]:\n",
    "        importances = rf.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({'feature': current_features, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "        top_features[dataset] = feature_importance_df['feature'].head(10).tolist()\n",
    "        print(f\"\\n{dataset} Top 10 Important Features:\\n\", feature_importance_df.head(10))\n",
    "    \n",
    "    # Predicted vs True RUL plot (optional, can be skipped to save time)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(true_rul['true_rul'], y_test_pred, alpha=0.5)\n",
    "    max_val = max(true_rul['true_rul'].max(), y_test_pred.max())\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "    plt.xlabel('True RUL')\n",
    "    plt.ylabel('Predicted RUL')\n",
    "    plt.title(f'Predicted vs True RUL on Test Set - {dataset}')\n",
    "    plt.savefig(f'predicted_vs_true_rul_{dataset}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c335f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project Summary Across Datasets\n",
      "\n",
      "Key Insights:\n",
      "- FD001/FD003 (single condition, single/dual fault) typically have higher R due to simpler patterns.\n",
      "- FD002/FD004 (multiple conditions, single/dual fault) are noisier, requiring operating condition clustering.\n",
      "- Random Forest generally outperforms Linear Regression and XGBoost across datasets.\n",
      "- Feature importance varies, with rolling means often critical (e.g., sensor_4_roll_mean in FD001).\n",
      "\n",
      "Next Steps:\n",
      "- Implement hyperparameter tuning for Random Forest/XGBoost in FD002/FD004 to handle noise.\n",
      "- Explore LSTM models for sequence modeling, leveraging TensorFlow.\n",
      "- Investigate low test R in FD002/FD004 (e.g., compare train-test feature distributions).\n",
      "- Experiment with different window sizes for rolling statistics in FD002/FD004.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProject Summary Across Datasets\")\n",
    "for dataset in datasets:\n",
    "    train_data = train_data_dict[dataset]\n",
    "    test_data = test_data_dict[dataset]\n",
    "    grouped_train = train_data.groupby(['unit_number', 'op_condition'])\n",
    "    grouped_test = test_data.groupby(['unit_number', 'op_condition'])\n",
    "    \n",
    "    new_train_cols = {}\n",
    "    new_test_cols = {}\n",
    "    for sensor in top_features[dataset]:  # Use top_features as defined earlier\n",
    "        # Ensure the sensor/feature exists in the grouped DataFrame\n",
    "        if sensor in train_data.columns and sensor in test_data.columns:\n",
    "            roll_stats_train = grouped_train[sensor].rolling(window=window, min_periods=1)\n",
    "            roll_stats_test = grouped_test[sensor].rolling(window=window, min_periods=1)\n",
    "            # Calculate rolling mean and align with original DataFrame index\n",
    "            new_train_cols[f'{sensor}_roll_mean'] = roll_stats_train.mean().fillna(train_data[sensor]).reset_index(drop=True)\n",
    "            new_test_cols[f'{sensor}_roll_mean'] = roll_stats_test.mean().fillna(test_data[sensor]).reset_index(drop=True)\n",
    "            # Calculate lagged features and align with original DataFrame index\n",
    "            new_train_cols[f'{sensor}_lag1'] = grouped_train[sensor].shift(1).fillna(train_data[sensor]).reset_index(drop=True)\n",
    "            new_train_cols[f'{sensor}_lag2'] = grouped_train[sensor].shift(2).fillna(train_data[sensor]).reset_index(drop=True)\n",
    "            new_test_cols[f'{sensor}_lag1'] = grouped_test[sensor].shift(1).fillna(test_data[sensor]).reset_index(drop=True)\n",
    "            new_test_cols[f'{sensor}_lag2'] = grouped_test[sensor].shift(2).fillna(test_data[sensor]).reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"Warning: Feature {sensor} not found in {dataset} data.\")\n",
    "    \n",
    "    # Update DataFrames with new columns\n",
    "    train_data = pd.concat([train_data, pd.DataFrame(new_train_cols, index=train_data.index)], axis=1)\n",
    "    test_data = pd.concat([test_data, pd.DataFrame(new_test_cols, index=test_data.index)], axis=1)\n",
    "    train_data_dict[dataset] = train_data\n",
    "    test_data_dict[dataset] = test_data\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- FD001/FD003 (single condition, single/dual fault) typically have higher R due to simpler patterns.\")\n",
    "print(\"- FD002/FD004 (multiple conditions, single/dual fault) are noisier, requiring operating condition clustering.\")\n",
    "print(\"- Random Forest generally outperforms Linear Regression and XGBoost across datasets.\")\n",
    "print(\"- Feature importance varies, with rolling means often critical (e.g., sensor_4_roll_mean in FD001).\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Implement hyperparameter tuning for Random Forest/XGBoost in FD002/FD004 to handle noise.\")\n",
    "print(\"- Explore LSTM models for sequence modeling, leveraging TensorFlow.\")\n",
    "print(\"- Investigate low test R in FD002/FD004 (e.g., compare train-test feature distributions).\")\n",
    "print(\"- Experiment with different window sizes for rolling statistics in FD002/FD004.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
